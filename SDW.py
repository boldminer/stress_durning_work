# -*- coding: utf-8 -*-
"""Copy of Final_Project_Team 1: Gregory Albarian, Guannan Liu, Yibo Yuan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y8orV8fcM34QhQLR4DS_hYm2reA46P2p

#Examining the Stress of Medical Practitioners

#Introduction





The dataset is the response collected from various medical partitioners to analyse their stress levels in the work space with the help of a questionnaire. The data is been collected from 1Lakh medical partitioners for 30 attributes who are working in private sector and public sector hospitals. Dataset contains the following attributes and the different options given to each question is mentioned in the brackets with the corresponding numerical representation in the dataset.

*should be added more*

#Load Packages
"""

import seaborn as sn
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.utils import resample
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve, roc_auc_score, auc
from sklearn.metrics import classification_report
from sklearn import tree
from sklearn import metrics
from sklearn.metrics import r2_score
import statsmodels.api as sm
from sklearn.metrics import accuracy_score as acc
from mlxtend.feature_selection import SequentialFeatureSelector as sfs
from statsmodels.stats.outliers_influence import variance_inflation_factor
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
from sklearn.ensemble import BaggingClassifier
from sklearn.datasets import make_classification
import numpy as np
from sklearn import datasets, ensemble
from sklearn.inspection import permutation_importance
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import RepeatedStratifiedKFold
import matplotlib.pyplot as plt
from sklearn import datasets, ensemble
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import train_test_split

"""#Read in dataset"""

dt=pd.read_csv('https://raw.githubusercontent.com/GregoryAlbarian/DataForCS530/main/data.csv')

"""## Target 1:Stressed_during_work"""

dt = dt.dropna() # Get off na 
dt = dt.replace({'Stressed_during_work' : { 1 : 2, 2 : 1}}) #change the label,

dt['Stressed_during_work'].value_counts() #The original data is unbalanced

#Resampling dataset to balance the dataset
dt_majority1 = dt[dt.Stressed_during_work==1]
dt_majority2 = dt[dt.Stressed_during_work==2]
dt_minority = dt[dt.Stressed_during_work==3]

dt_majority1_downsampled = resample(dt_majority1,replace=False,n_samples=6662,random_state=42)
dt_minority2_downsampled = resample(dt_majority2,replace=False,n_samples=6662,random_state=42)
dt_balanced = pd.concat([dt_majority1_downsampled,dt_minority2_downsampled,dt_minority])

dt_balanced['Stressed_during_work'].value_counts()

dt_balanced.info()

#Creating function for automatically detect and drop features by VIF
def calculate_vif_(X, thresh=5):
    variables = list(range(X.shape[1]))
    dropped = True
    while dropped:
        dropped = False
        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)
               for ix in range(X.iloc[:, variables].shape[1])]

        maxloc = vif.index(max(vif))
        if max(vif) > thresh:
            print('dropping \'' + X.iloc[:, variables].columns[maxloc] +
                  '\' at index: ' + str(maxloc))
            del variables[maxloc]
            dropped = True

    print('Remaining variables:')
    print(X.columns[variables])
    return X.iloc[:, variables]

#Dropping features based on VIF with threshold of VIF <= 5
stressed_during_work_features = dt_balanced.astype('category').drop(['Stressed_during_work'], axis = 1)
X1 = calculate_vif_(stressed_during_work_features)

#VIF after some of the features have been selected out based on previous VIF dropping
vif_data_target1 = pd.DataFrame()
vif_data_target1["feature"] = X1.columns
vif_data_target1["VIF"] = [variance_inflation_factor(X1.values, i)
                         for i in range(len(X1.columns))]

print(vif_data_target1)

#Create dummies for features selected and split dataset into train and test set
categorical_columns = X1.loc[:, X1.columns].columns.tolist()
for col in categorical_columns:
    col_ohe = pd.get_dummies(X1[col], prefix=col)
    X1 = pd.concat((X1, col_ohe), axis=1).drop(col, axis=1)


y = dt_balanced.astype('category').loc[:, "Stressed_during_work"]

X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y, test_size=0.3, random_state = 42)

lm = LinearRegression()

# Build step forward feature selection
sfs1 = sfs(lm,
           k_features='best',
           forward=True,
           floating=False,
           verbose=2,
           scoring='r2',
           cv=5)

# Perform SFFS
sfs1 = sfs1.fit(X_train1, y_train1)

#Plot of Foward Sequential Feature Selection with R squared performance in association with number of features selected
fig1 = plot_sfs(sfs1.get_metric_dict(),
                kind='std_dev')

plt.title('Sequential Forward Selection (w. StdDev)')
plt.grid()
plt.show()

#Print the result of Forward Sequential Feature Selection
feat_cols = list(sfs1.k_feature_names_)
print('Ideal number of features: %.2f' % len(sfs1.k_feature_idx_)) 
print('Best R2 score: %.2f' % sfs1.k_score_)  
print('Best subset (indices):', sfs1.k_feature_idx_)                                        
print('Best subset (corresponding names):', *feat_cols,sep="\n")

#Linear model with all original variables
x_all = dt_balanced.astype('category').drop(['Stressed_during_work'], axis = 1)
y_all = dt_balanced.astype('category').loc[:, "Stressed_during_work"]
cc = x_all.loc[:, x_all.columns].columns.tolist()
for col in cc:
    col_all = pd.get_dummies(x_all[col], prefix=col)
    x_all = pd.concat((x_all, col_all), axis=1).drop(col, axis=1)

X_train_all_target1, X_test_all_target1, y_train_all_target1, y_test_all_target1 = train_test_split(x_all, y_all, test_size=0.3, random_state = 42)

lm1 = LinearRegression()
lm1.fit(X_train_all_target1,y_train_all_target1)
pred_all_target1 = lm1.predict(X_test_all_target1)
print("R^2: {}".format(round(lm1.score(X_test_all_target1, y_test_all_target1),2)))
print("MSE: {}".format(round(metrics.mean_squared_error(y_test_all_target1,pred_all_target1),2)))

X_train_all_target1.shape

X_train1.shape

#Linear model with stepwise selected variables based on SequentialFeatureSelector
X_key_train = X_train1[feat_cols]
X_key_test = X_test1[feat_cols]
lm3 = LinearRegression()
lm3.fit(X_key_train,y_train1)
pred_key = lm3.predict(X_key_test)
print("R^2: {}".format(round(lm3.score(X_key_test, y_test1),2)))
print("MSE: {}".format(round(metrics.mean_squared_error(y_test1,pred_key),2)))

X_key_train.shape # end of Yibo's code of target 1

#Guannan on target 1

clf = svm.SVC(kernel='linear', C=1, random_state=66)
scores = cross_val_score(clf, X_train1, y_train1, cv=5)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

#run RF\
a=[]
for i in range(1,7,1):
    clf = RandomForestClassifier(max_depth=i, random_state=0)
    clf.fit(X_train1, y_train1)
    clf.score(X_test1, y_test1, sample_weight=None)
    predictions = clf.predict(X_test1)  #predict test
    lable=np.asarray(y_test1)
    good=np.sum(lable == predictions)
    accuracy=good/len(lable)
    a.append(accuracy)
print(a)

num_trees=[1,2,3,4,5,6]
plt.bar(num_trees,a)
plt.xlabel("Max_depth")
plt.title('Accuracy of different Tree depth')

sorted_idx = clf.feature_importances_.argsort()
plt.barh(X_train1.columns[sorted_idx], clf.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance") 
# with balanced dat

#GridSearchCV
param_grid = {
                 'n_estimators': [5, 10, 15, 20],
                 'max_depth': [2, 5, 7, 9]
             }
from sklearn.model_selection import GridSearchCV

grid_clf = GridSearchCV(clf, param_grid, cv=10)
grid_clf.fit(X_train1, y_train1)
grid_clf.cv_results_

#BAGGING with svc
clf = BaggingClassifier(base_estimator=SVC(),#SVC 
                        n_estimators=5, random_state=0).fit(X_train1, y_train1)
predictions = clf.predict(X_test1)  #predict test
lable=np.asarray(y_test1)
good=np.sum(lable == predictions)
accuracy=good/len(lable)
print('without cv, the accuracy is :',accuracy)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(clf, X_train1, y_train1, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

#BAGGING with DecisionTreeClassifier

clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),#SVC 
                        n_estimators=5, random_state=0).fit(X_train1, y_train1)
predictions = clf.predict(X_test1)  #predict test
lable=np.asarray(y_test1)
good=np.sum(lable == predictions)
accuracy=good/len(lable)
print('without cv, the accuracy is :',accuracy)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(clf, X_train1, y_train1, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

# Gradient Boosting regression¶
#While boosting can increase the accuracy of a base learner, such as a decision tree or linear regression, it sacrifices intelligibility and interpretability

params = {'n_estimators': 500,
          'max_depth': 4,
          'min_samples_split': 5,
          'learning_rate': 0.01,
          'loss': 'ls'}
reg = ensemble.GradientBoostingRegressor(**params)
reg.fit(X_train1, y_train1)

mse = mean_squared_error(y_test1, reg.predict(X_test1))

print("R^2: {}".format(round(reg.score(X_test1, y_test1),2)))
print("The mean squared error (MSE) on test set: {:.4f}".format(mse),'\n\n\n')

reg.score(X_test1, y_test1)

feature_importance = reg.feature_importances_
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
fig = plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, np.array(X_train1.columns)[sorted_idx])
plt.title('Feature Importance (MDI)')

result = permutation_importance(reg, X_test1, y_test1, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
plt.subplot(1, 2, 2)
plt.boxplot(result.importances[sorted_idx].T,
            vert=False, labels=np.array(X_train1.columns)[sorted_idx])
plt.title("Permutation Importance (test set)")
fig.tight_layout()
plt.show() 

## Gradient Boosting regression shows the overtime work paid is the most important

"""## Target 2:Feeling_on_job"""

dt.info()

dt['Feeling_on_job'].value_counts()

#Resampling dataset to balance the dataset
dt2_majority1 = dt[dt.Feeling_on_job==1]
dt2_majority2 = dt[dt.Feeling_on_job==2]
dt2_minority1 = dt[dt.Feeling_on_job==3]
dt2_minority2 = dt[dt.Feeling_on_job==4]

dt2_majority1_downsampled = resample(dt2_majority1,replace=False,n_samples=3318,random_state=42)
dt2_majority2_downsampled = resample(dt2_majority2,replace=False,n_samples=3318,random_state=42)
dt2_minority2_upsampled = resample(dt2_minority2,replace=True,n_samples=3318,random_state=42)
dt2_balanced = pd.concat([dt2_majority1_downsampled,dt2_majority2_downsampled,dt2_minority2_upsampled,dt2_minority1])

dt2_balanced['Feeling_on_job'].value_counts()

dt2_balanced.info()

#Dropping features based on VIF with threshold of VIF <= 5
Feeling_on_job_features = dt2_balanced.astype('category').drop(['Feeling_on_job'], axis = 1)
X2 = calculate_vif_(Feeling_on_job_features,thresh=10)

#VIF after some of the features have been selected out based on previous VIF dropping
vif_data_target2 = pd.DataFrame()
vif_data_target2["feature"] = X2.columns
vif_data_target2["VIF"] = [variance_inflation_factor(X2.values, i)
                         for i in range(len(X2.columns))]
  
print(vif_data_target2)

#Create dummies for features selected and split dataset into train and test set
categorical_columns2 = X2.loc[:, X2.columns].columns.tolist()
for col in categorical_columns2:
    col_ohe2 = pd.get_dummies(X2[col], prefix=col)
    X2 = pd.concat((X2, col_ohe2), axis=1).drop(col, axis=1)


y2 = dt2_balanced.astype('category').loc[:, "Feeling_on_job"]

X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state = 42)

lr2 = LinearRegression()

# Build step forward feature selection
sfs2 = sfs(lr2,
           k_features='best',
           forward=True,
           floating=False,
           verbose=2,
           scoring='r2',
           cv=5)

# Perform SFFS
sfs2 = sfs2.fit(X_train2, y_train2)

#Plot of Foward Sequential Feature Selection with R squared performance in association with number of features selected
fig2 = plot_sfs(sfs2.get_metric_dict(),
                kind='std_dev')

plt.title('Sequential Forward Selection (w. StdDev)')
plt.grid()
plt.show()

#Print the result of Forward Sequential Feature Selection
feat_cols2 = list(sfs2.k_feature_names_)
print('Ideal number of features: %.2f' % len(sfs2.k_feature_idx_)) 
print('Best R2 score: %.2f' % sfs2.k_score_) 
print('Best subset (indices):', sfs2.k_feature_idx_)                                        
print('Best subset (corresponding names):', *feat_cols2,sep="\n")

#Linear model with all original variables
x_all2 = dt2_balanced.astype('category').drop(['Feeling_on_job'], axis = 1)
y_all2 = dt2_balanced.astype('category').loc[:, "Feeling_on_job"]
cc2 = x_all2.loc[:, x_all2.columns].columns.tolist()
for col in cc2:
    col_all2 = pd.get_dummies(x_all2[col], prefix=col)
    x_all2 = pd.concat((x_all2, col_all2), axis=1).drop(col, axis=1)

X_train_all_target2, X_test_all_target2, y_train_all_target2, y_test_all_target2 = train_test_split(x_all2, y_all2, test_size=0.3, random_state = 42)

lm2_1 = LinearRegression()
lm2_1.fit(X_train_all_target2,y_train_all_target2)
pred_all_target2 = lm2_1.predict(X_test_all_target2)
print("R^2: {}".format(round(lm2_1.score(X_test_all_target2, y_test_all_target2),2)))
print("MSE: {}".format(round(metrics.mean_squared_error(y_test_all_target2,pred_all_target2),2)))

X_train_all_target2.shape

#Linear model with all VIF selected variables
lm2_2 = LinearRegression()
lm2_2.fit(X_train2,y_train2)
pred_V_target2 = lm2_2.predict(X_test2)
print("R^2: {}".format(round(lm2_2.score(X_test2, y_test2),2)))
print("MSE: {}".format(round(metrics.mean_squared_error(y_test2,pred_V_target2),2)))

X_train2.shape

#Linear model with stepwise selected variables based on SequentialFeatureSelector
X_key_train2 = X_train2[feat_cols2]
X_key_test2 = X_test2[feat_cols2]
lm2_3 = LinearRegression()
lm2_3.fit(X_key_train2,y_train2)
pred_key2 = lm2_3.predict(X_key_test2)
print("R^2: {}".format(round(lm2_3.score(X_key_test2, y_test2),2)))
print("MSE: {}".format(round(metrics.mean_squared_error(y_test2,pred_key2),2)))

X_key_train2.shape

# this is Guannan's part of target2
#svm
clf = svm.SVC(kernel='linear', C=1, random_state=42)
scores = cross_val_score(clf, X_train2, y_train2, cv=5)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

#run RF\
a=[]
for i in range(1,9,1):
    clf = RandomForestClassifier(max_depth=i, random_state=0)
    clf.fit(X_train2, y_train2)
    clf.score(X_test2, y_test2, sample_weight=None)
    predictions = clf.predict(X_test2)  #predict test
    lable=np.asarray(y_test2)
    good=np.sum(lable == predictions)
    accuracy=good/len(lable)
    a.append(accuracy)
print(a)

num_trees=[1,2,3,4,5,6,7,8]
plt.bar(num_trees,a)
plt.xlabel("Max_depth")
plt.title('Accuracy of different Tree depth')

sorted_idx = clf.feature_importances_.argsort()
plt.barh(X_train2.columns[sorted_idx], clf.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance") 
# with balanced dat

#GridSearchCV
param_grid = {
                 'n_estimators': [5, 10, 15, 20],
                 'max_depth': [5, 7, 9]
             }
from sklearn.model_selection import GridSearchCV

grid_clf = GridSearchCV(clf, param_grid, cv=5)
grid_clf.fit(X_train2, y_train2)
grid_clf.cv_results_

#BAGGING with svc
clf = BaggingClassifier(base_estimator=SVC(),#SVC 
                        n_estimators=5, random_state=0).fit(X_train2, y_train2)
predictions = clf.predict(X_test2)  #predict test
lable=np.asarray(y_test2)
good=np.sum(lable == predictions)
accuracy=good/len(lable)
print('without cv, the accuracy is :',accuracy)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(clf, X_train2, y_train2, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

#BAGGING with DecisionTreeClassifier

clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),#SVC 
                        n_estimators=5, random_state=0).fit(X_train2, y_train2)
predictions = clf.predict(X_test2)  #predict test
lable=np.asarray(y_test2)
good=np.sum(lable == predictions)
accuracy=good/len(lable)
print('without cv, the accuracy is :',accuracy)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(clf, X_train2, y_train2, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

# Gradient Boosting regression¶
#While boosting can increase the accuracy of a base learner, such as a decision tree or linear regression, it sacrifices intelligibility and interpretability

params = {'n_estimators': 500,
          'max_depth': 4,
          'min_samples_split': 5,
          'learning_rate': 0.01,
          'loss': 'ls'}
reg = ensemble.GradientBoostingRegressor(**params)
reg.fit(X_train2, y_train2)

mse = mean_squared_error(y_test2, reg.predict(X_test2))

print("R^2: {}".format(round(reg.score(X_test2, y_test2),2)))
print("The mean squared error (MSE) on test set: {:.4f}".format(mse),'\n\n\n')

feature_importance = reg.feature_importances_
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
fig = plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, np.array(X_train2.columns)[sorted_idx])
plt.title('Feature Importance (MDI)')

result = permutation_importance(reg, X_test2, y_test2, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
plt.subplot(1, 2, 2)
plt.boxplot(result.importances[sorted_idx].T,
            vert=False, labels=np.array(X_train2.columns)[sorted_idx])
plt.title("Permutation Importance (test set)")
fig.tight_layout()
plt.show() 

## Gradient Boosting regression shows the overtime work paid is the most important

reg.score(X_test2, y_test2)

"""## Target3:Stress_affecting_patient_care"""

dt.info()

dt['Stress_affecting_patient_care'].value_counts()

#Resampling dataset to balance the dataset
dt3_majority1 = dt[dt.Stress_affecting_patient_care==2]
dt3_minority1 = dt[dt.Stress_affecting_patient_care==1]

dt3_majority1_downsampled = resample(dt3_majority1,replace=False,n_samples=39978,random_state=42)
dt3_balanced = pd.concat([dt3_majority1_downsampled,dt3_minority1])

dt3_balanced['Stress_affecting_patient_care'].value_counts()

dt3_balanced.info()

#Dropping features based on VIF with threshold of VIF <= 5
Stress_affecting_patient_care_features = dt3_balanced.astype('category').drop(['Stress_affecting_patient_care'], axis = 1)
X3 = calculate_vif_(Stress_affecting_patient_care_features)

#VIF after some of the features have been selected out based on previous VIF dropping
vif_data_target3 = pd.DataFrame()
vif_data_target3["feature"] = X3.columns
vif_data_target3["VIF"] = [variance_inflation_factor(X3.values, i)
                         for i in range(len(X3.columns))]

print(vif_data_target3)

#Create dummies for features selected and split dataset into train and test set
categorical_columns3 = X3.loc[:, X3.columns].columns.tolist()
for col in categorical_columns3:
    col_ohe3 = pd.get_dummies(X3[col], prefix=col)
    X3 = pd.concat((X3, col_ohe3), axis=1).drop(col, axis=1)


y3 = dt3_balanced.astype('category').loc[:, "Stress_affecting_patient_care"]

X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.3, random_state = 42)

lr3 = LinearRegression()

# Build step forward feature selection
sfs3 = sfs(lr3,
           k_features='best',
           forward=True,
           floating=False,
           verbose=2,
           scoring='r2',
           cv=5)

# Perform SFFS
sfs3 = sfs3.fit(X_train3, y_train3)

#Plot of Foward Sequential Feature Selection with R squared performance in association with number of features selected
fig3 = plot_sfs(sfs3.get_metric_dict(),
                kind='std_dev')

plt.title('Sequential Forward Selection (w. StdDev)')
plt.grid()
plt.show()

#Print the result of Forward Sequential Feature Selection
feat_cols3 = list(sfs3.k_feature_names_)
print('Ideal number of features: %.2f' % len(sfs3.k_feature_idx_)) 
print('Best R2 score: %.2f' % sfs3.k_score_) 
print('Best subset (indices):', sfs3.k_feature_idx_)                                        
print('Best subset (corresponding names):', *feat_cols3,sep="\n")

#Table of outcome of Forward Sequential Feature Selection
pd.DataFrame.from_dict(sfs3.get_metric_dict()).T

#Linear model with all original variables
x_all3 = dt3_balanced.astype('category').drop(['Stress_affecting_patient_care'], axis = 1)
y_all3 = dt3_balanced.astype('category').loc[:, "Stress_affecting_patient_care"]
cc3 = x_all3.loc[:, x_all3.columns].columns.tolist()
for col in cc3:
    col_all3 = pd.get_dummies(x_all3[col], prefix=col)
    x_all3 = pd.concat((x_all3, col_all3), axis=1).drop(col, axis=1)

X_train_all_target3, X_test_all_target3, y_train_all_target3, y_test_all_target3 = train_test_split(x_all3, y_all3, test_size=0.3, random_state = 42)

lm3_1 = LinearRegression()
lm3_1.fit(X_train_all_target3,y_train_all_target3)
pred_all_target3 = lm3_1.predict(X_test_all_target3)
print("R^2: {}".format(round(lm3_1.score(X_test_all_target3, y_test_all_target3),2)))
print("MSE: {}".format(round(metrics.mean_squared_error(y_test_all_target3,pred_all_target3),2)))

X_train_all_target3.shape

#Linear model with all VIF selected variables
lm3_2 = LinearRegression()
lm3_2.fit(X_train3,y_train3)
pred_V_target3 = lm3_2.predict(X_test3)
print("R^2: {}".format(round(lm3_2.score(X_test3, y_test3),2)))
print("MSE: {}".format(round(metrics.mean_squared_error(y_test3,pred_V_target3),2)))

X_train3.shape

#Linear model with stepwise selected variables based on SequentialFeatureSelector
X_key_train3 = X_train3[feat_cols3]
X_key_test3 = X_test3[feat_cols3]
lm3_3 = LinearRegression()
lm3_3.fit(X_key_train3,y_train3)
pred_key3 = lm3_3.predict(X_key_test3)
print("R^2: {}".format(round(lm3_3.score(X_key_test3, y_test3),2)))
print("MSE: {}".format(round(metrics.mean_squared_error(y_test3,pred_key3),2)))

X_key_train3.shape

#Guannan on target 3

clf = svm.SVC(kernel='linear', C=1, random_state=42)
scores = cross_val_score(clf, X_train3, y_train3, cv=5)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

#run RF\
a=[]
for i in range(1,6,1):
    clf = RandomForestClassifier(max_depth=i, random_state=0)
    clf.fit(X_train3, y_train3)
    clf.score(X_test3, y_test3, sample_weight=None)
    predictions = clf.predict(X_test3)  #predict test
    lable=np.asarray(y_test3)
    good=np.sum(lable == predictions)
    accuracy=good/len(lable)
    a.append(accuracy)
print(a)

num_trees=[1,2,3,4,5]
plt.bar(num_trees,a)
plt.xlabel("Max_depth")
plt.title('Accuracy of different Tree depth')

sorted_idx = clf.feature_importances_.argsort()
plt.barh(X_train3.columns[sorted_idx], clf.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance") 
# with balanced dat

#GridSearchCV
param_grid = {
                 'n_estimators': [5, 10, 15, 20],
                 'max_depth': [2, 5, 7, 9]
             }
from sklearn.model_selection import GridSearchCV

grid_clf = GridSearchCV(clf, param_grid, cv=10)
grid_clf.fit(X_train3, y_train3)
grid_clf.cv_results_

#BAGGING with svc
clf = BaggingClassifier(base_estimator=SVC(),#SVC 
                        n_estimators=5, random_state=0).fit(X_train3, y_train3)
predictions = clf.predict(X_test3)  #predict test
lable=np.asarray(y_test3)
good=np.sum(lable == predictions)
accuracy=good/len(lable)
print('without cv, the accuracy is :',accuracy)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(clf, X_train3, y_train3, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

#BAGGING with DecisionTreeClassifier
clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),#SVC 
                        n_estimators=5, random_state=0).fit(X_train3, y_train3)
predictions = clf.predict(X_test3)  #predict test
lable=np.asarray(y_test3)
good=np.sum(lable == predictions)
accuracy=good/len(lable)
print('without cv, the accuracy is :',accuracy)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(clf, X_train3, y_train3, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

# Gradient Boosting regression¶
#While boosting can increase the accuracy of a base learner, such as a decision tree or linear regression, it sacrifices intelligibility and interpretability

params = {'n_estimators': 500,
          'max_depth': 4,
          'min_samples_split': 5,
          'learning_rate': 0.01,
          'loss': 'ls'}
reg = ensemble.GradientBoostingRegressor(**params)
reg.fit(X_train3, y_train3)

mse = mean_squared_error(y_test3, reg.predict(X_test3))

print("R^2: {}".format(round(reg.score(X_test3, y_test3),2)))
print("The mean squared error (MSE) on test set: {:.4f}".format(mse),'\n\n\n')

feature_importance = reg.feature_importances_
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
fig = plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, np.array(X_train3.columns)[sorted_idx])
plt.title('Feature Importance (MDI)')

result = permutation_importance(reg, X_test3, y_test3, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
plt.subplot(1, 2, 2)
plt.boxplot(result.importances[sorted_idx].T,
            vert=False, labels=np.array(X_train3.columns)[sorted_idx])
plt.title("Permutation Importance (test set)")
fig.tight_layout()
plt.show() 

## Gradient Boosting regression shows the overtime work paid is the most important

reg.score(X_test3, y_test3)

#end of Guannan's part





"""# Neural Network"""

# Use scikit-learn to grid search the batch size and epochs
# import numpy as np
# from sklearn.model_selection import GridSearchCV

def neural_network_3_layer(optimizers='Adam', neurons=20, dropout_rate=0.2):

  model = Sequential()
  model.add(Dense(neurons, input_shape=(29,), activation="relu"))
  model.add(Dropout(dropout_rate))
  model.add(Dense(1, activation="linear"))
  model.compile(loss='mean_squared_error', optimizer=optimizers, metrics=['mean_squared_error'])

  return model

def neural_network(X, y):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)

  model = neural_network_3_layer()

  model.fit(X_train, y_train, epochs = 40, batch_size=25, verbose = 0)
  _, accuracy = model.evaluate(X_test, y_test, verbose = 0)

  print("mean squared error: ", accuracy)

y1 = dt_balanced["Stressed_during_work"]

X1 = dt_balanced.drop("Stressed_during_work", axis = 1)
# y = pd.get_dummies(y)
neural_network(X1, y1)

y2 = dt2_balanced["Feeling_on_job"]

X2 = dt2_balanced.drop("Feeling_on_job", axis = 1)
# y = pd.get_dummies(y)
neural_network(X2, y2)

y3 = dt3_balanced["Stress_affecting_patient_care"]

X3 = dt3_balanced.drop("Stress_affecting_patient_care", axis = 1)
# y = pd.get_dummies(y)
neural_network(X3, y3)

"""# Permutation Test ideas"""

# use dt1, dt2, dt3 or so but unbalanced
# from mlxtend.evaluate import permutation_test

print("original dataframe shape = ", dt.shape)

print("first cleaned dataframe shape = ", dt_balanced.shape)

print("second cleaned dataframe shape = ", dt2_balanced.shape)

print("first cleaned dataframe shape = ", dt3_balanced.shape)

"""As one may see, there are not many biases in the sample set created because there were tons of permutations that were possible to sample from. We did not handpick these naturally in the beginning and used a functions that would help us randomize the process.

# **Conclusion**

Overall, we got very high accuracy rates throughout our program. Even though, we did more and more sophisticate model throughout our tests it didn't alway feel like we needed to. SVMs and ensemble tree algorithms did the best. It was difficult to compare the neural networks because it could take a long time to code multiple combinations on hardware that was not as accessible. This could have felt fishy how accurate our tests were but the we used cross validations and  ran out programs many times. Overall, we will able to successfully predict our key target variables.
"""